# 38-ç›®æ ‡æ£€æµ‹ç®—æ³•R-CNNä»‹ç»

ä½œè€…ï¼šé«˜é›¨èŒ

## ç›®æ ‡æ£€æµ‹ç®€ä»‹
ç›®æ ‡æ£€æµ‹ï¼ˆObject Detectionï¼‰çš„ä»»åŠ¡æ˜¯æ‰¾å‡ºå›¾åƒä¸­æ‰€æœ‰æ„Ÿå…´è¶£çš„ç›®æ ‡ï¼ˆç‰©ä½“ï¼‰ï¼Œç¡®å®šå®ƒä»¬çš„ç±»åˆ«å’Œä½ç½®ã€‚
è®¡ç®—æœºè§†è§‰ä¸­å…³äºå›¾åƒè¯†åˆ«æœ‰å››å¤§ç±»ä»»åŠ¡ï¼š
1.åˆ†ç±»-Classificationï¼šè§£å†³â€œæ˜¯ä»€ä¹ˆï¼Ÿâ€çš„é—®é¢˜ï¼Œå³ç»™å®šä¸€å¼ å›¾ç‰‡æˆ–ä¸€æ®µè§†é¢‘åˆ¤æ–­é‡Œé¢åŒ…å«ä»€ä¹ˆç±»åˆ«çš„ç›®æ ‡ã€‚
2.å®šä½-Locationï¼šè§£å†³â€œåœ¨å“ªé‡Œï¼Ÿâ€çš„é—®é¢˜ï¼Œå³å®šä½å‡ºè¿™ä¸ªç›®æ ‡çš„çš„ä½ç½®ã€‚
3.æ£€æµ‹-Detectionï¼šè§£å†³â€œæ˜¯ä»€ä¹ˆï¼Ÿåœ¨å“ªé‡Œï¼Ÿâ€çš„é—®é¢˜ï¼Œå³å®šä½å‡ºè¿™ä¸ªç›®æ ‡çš„çš„ä½ç½®å¹¶ä¸”çŸ¥é“ç›®æ ‡ç‰©æ˜¯ä»€ä¹ˆã€‚
4.åˆ†å‰²-Segmentationï¼šåˆ†ä¸ºå®ä¾‹çš„åˆ†å‰²ï¼ˆInstance-levelï¼‰å’Œåœºæ™¯åˆ†å‰²ï¼ˆScene-levelï¼‰ï¼Œè§£å†³â€œæ¯ä¸€ä¸ªåƒç´ å±äºå“ªä¸ªç›®æ ‡ç‰©æˆ–åœºæ™¯â€çš„é—®é¢˜ã€‚
![OD.png](https://cdn.nlark.com/yuque/0/2020/png/1626932/1596503017382-9a1af278-e95b-457c-89ff-580b4ec966c8.png#align=left&display=inline&height=487&margin=%5Bobject%20Object%5D&name=OD.png&originHeight=487&originWidth=1176&size=774284&status=done&style=none&width=1176#align=left&display=inline&height=487&margin=%5Bobject%20Object%5D&originHeight=487&originWidth=1176&status=done&style=none&width=1176)


## å½“å‰ç›®æ ‡æ£€æµ‹ç®—æ³•åˆ†ç±»
1.Two stageç›®æ ‡æ£€æµ‹ç®—æ³•
å…ˆè¿›è¡ŒåŒºåŸŸç”Ÿæˆï¼ˆregion proposalï¼ŒRPï¼‰ï¼ˆä¸€ä¸ªæœ‰å¯èƒ½åŒ…å«å¾…æ£€ç‰©ä½“çš„é¢„é€‰æ¡†ï¼‰ï¼Œå†é€šè¿‡å·ç§¯ç¥ç»ç½‘ç»œè¿›è¡Œæ ·æœ¬åˆ†ç±»ã€‚
ä»»åŠ¡ï¼šç‰¹å¾æå–â€”>ç”ŸæˆRPâ€”>åˆ†ç±»/å®šä½å›å½’ã€‚
å¸¸è§çš„two stageç›®æ ‡æ£€æµ‹ç®—æ³•æœ‰ï¼šR-CNNã€SPP-Netã€Fast R-CNNã€Faster R-CNNå’ŒR-FCNç­‰ã€‚


2.One stageç›®æ ‡æ£€æµ‹ç®—æ³•
ä¸ç”¨RPï¼Œç›´æ¥åœ¨ç½‘ç»œä¸­æå–ç‰¹å¾æ¥é¢„æµ‹ç‰©ä½“åˆ†ç±»å’Œä½ç½®ã€‚
ä»»åŠ¡ï¼šç‰¹å¾æå–â€”>åˆ†ç±»/å®šä½å›å½’ã€‚
å¸¸è§çš„one stageç›®æ ‡æ£€æµ‹ç®—æ³•æœ‰ï¼šOverFeatã€YOLOv1ã€YOLOv2ã€YOLOv3ã€SSDå’ŒRetinaNetç­‰ã€‚

æœ¬æ–‡åç»­å°†ä»‹ç»å…¶ä¸­çš„ç»å…¸ç®—æ³•**R-CNN**å¹¶ç»™å‡ºç›¸åº”çš„ä»£ç å®ç°ã€‚


# R-CNN
R-CNNï¼ˆRegions with CNN featuresï¼‰æ˜¯å°†CNNæ–¹æ³•åº”ç”¨åˆ°ç›®æ ‡æ£€æµ‹é—®é¢˜ä¸Šçš„ä¸€ä¸ªé‡Œç¨‹ç¢‘ã€‚å€ŸåŠ©CNNè‰¯å¥½çš„ç‰¹å¾æå–å’Œåˆ†ç±»æ€§èƒ½ï¼Œé€šè¿‡RegionProposalæ–¹æ³•å®ç°ç›®æ ‡æ£€æµ‹é—®é¢˜çš„è½¬åŒ–ã€‚
**ç®—æ³•åˆ†ä¸ºå››ä¸ªæ­¥éª¤ï¼š**

1. ä»åŸå›¾åƒç”Ÿæˆå€™é€‰åŒºåŸŸ(RoI proposal)
2. å°†å€™é€‰åŒºåŸŸè¾“å…¥CNNè¿›è¡Œç‰¹å¾æå–
3. å°†ç‰¹å¾é€å…¥æ¯ä¸€ç±»åˆ«çš„SVMæ£€æµ‹å™¨ï¼Œåˆ¤æ–­æ˜¯å¦å±äºè¯¥ç±»
4. é€šè¿‡è¾¹ç•Œå›å½’å¾—åˆ°ç²¾ç¡®çš„ç›®æ ‡åŒºåŸŸ

ç®—æ³•å‰å‘æµç¨‹å›¾å¦‚ä¸‹ï¼ˆå›¾ä¸­æ•°å­—æ ‡è®°å¯¹åº”ä¸Šè¿°å››ä¸ªæ­¥éª¤ï¼‰ï¼š
![RCNN_.png](https://cdn.nlark.com/yuque/0/2020/png/1626932/1596503348145-a1acfed7-74a0-4788-aee0-d2f768ed8499.png#align=left&display=inline&height=458&margin=%5Bobject%20Object%5D&name=RCNN_.png&originHeight=458&originWidth=664&size=169146&status=done&style=none&width=664#align=left&display=inline&height=458&margin=%5Bobject%20Object%5D&originHeight=458&originWidth=664&status=done&style=none&width=664)
åœ¨ä¸‹æ–‡ä¸­æˆ‘ä»¬ä¹Ÿä¼šæŒ‰ç…§ä¸Šè¿°å››ä¸ªæ­¥éª¤çš„é¡ºåºè®²è§£**æ¨¡å‹æ„å»º**ï¼Œåœ¨è¿™ä¹‹åæˆ‘ä»¬ä¼šè®²è§£å¦‚ä½•è¿›è¡Œ**æ¨¡å‹è®­ç»ƒ**ã€‚
ä½†åœ¨å¼€å§‹å…·ä½“ä¸Šè¿°æ“ä½œä¹‹å‰ï¼Œè®©æˆ‘ä»¬ç®€å•äº†è§£ä¸‹åœ¨è®­ç»ƒä¸­æˆ‘ä»¬å°†ä¼šä½¿ç”¨åˆ°çš„æ•°æ®é›†ã€‚

## 
## æ•°æ®é›†ç®€ä»‹
åŸè®ºæ–‡ä¸­ä½¿ç”¨çš„æ•°æ®é›†ä¸ºï¼š
1.ImageNet ILSVCï¼ˆä¸€ä¸ªè¾ƒå¤§çš„è¯†åˆ«åº“ï¼‰ **ä¸€åƒä¸‡å›¾åƒï¼Œ1000ç±»ã€‚**
2.PASCAL VOC 2007ï¼ˆä¸€ä¸ªè¾ƒå°çš„æ£€æµ‹åº“ï¼‰ **ä¸€ä¸‡å›¾åƒï¼Œ20ç±»ã€‚**
è®­ç»ƒæ—¶ä½¿ç”¨è¯†åˆ«åº“è¿›è¡Œé¢„è®­ç»ƒï¼Œè€Œåç”¨æ£€æµ‹åº“è°ƒä¼˜å‚æ•°å¹¶åœ¨æ£€æµ‹åº“ä¸Šè¯„æµ‹æ¨¡å‹æ•ˆæœã€‚

ç”±äºåŸæ•°æ®é›†å®¹é‡è¾ƒå¤§ï¼Œæ¨¡å‹çš„è®­ç»ƒæ—¶é—´å¯èƒ½ä¼šè¾¾åˆ°å‡ åä¸ªå°æ—¶ä¹‹ä¹…ã€‚ä¸ºäº†ç®€åŒ–è®­ç»ƒï¼Œæˆ‘ä»¬æ›¿æ¢äº†è®­ç»ƒæ•°æ®é›†ã€‚
ä¸åŸè®ºæ–‡ç±»ä¼¼ï¼Œæˆ‘ä»¬ä½¿ç”¨çš„æ•°æ®åŒ…æ‹¬ä¸¤éƒ¨åˆ†ï¼š
1.å«**17ç§åˆ†ç±»**çš„èŠ±æœµå›¾ç‰‡ 
2.å«**2ç§åˆ†ç±»**çš„èŠ±æœµå›¾ç‰‡ã€‚

æˆ‘ä»¬åç»­å°†ä½¿ç”¨17åˆ†ç±»æ•°æ®è¿›è¡Œæ¨¡å‹çš„é¢„è®­ç»ƒï¼Œç”¨2åˆ†ç±»æ•°æ®è¿›è¡Œfine-tuningå¾—åˆ°æœ€ç»ˆçš„é¢„æµ‹æ¨¡å‹,å¹¶åœ¨2åˆ†ç±»å›¾ç‰‡ä¸Šè¿›è¡Œè¯„æµ‹ã€‚


## æ¨¡å‹æ„å»º
### æ­¥éª¤ä¸€
è¯¥æ­¥éª¤ä¸­æˆ‘ä»¬è¦å®Œæˆçš„ç®—æ³•æµç¨‹éƒ¨åˆ†å¦‚ä¸‹å›¾æ•°å­—æ ‡è®°ï¼š
![RCNN_1.png](https://cdn.nlark.com/yuque/0/2020/png/1626932/1596503572239-3fca17db-c182-4c09-8c20-add3ae52a627.png#align=left&display=inline&height=458&margin=%5Bobject%20Object%5D&name=RCNN_1.png&originHeight=458&originWidth=664&size=168588&status=done&style=none&width=664#align=left&display=inline&height=458&margin=%5Bobject%20Object%5D&originHeight=458&originWidth=664&status=done&style=none&width=664)
R-CNNä¸­é‡‡ç”¨äº†**selective searchç®—æ³•**æ¥è¿›è¡Œ**region proposal**ã€‚è¯¥ç®—æ³•é¦–å…ˆé€šè¿‡åŸºäºå›¾çš„å›¾åƒåˆ†å‰²æ–¹æ³•åˆå§‹åŒ–åŸå§‹åŒºåŸŸï¼Œå³å°†å›¾åƒåˆ†å‰²æˆå¾ˆå¤šå¾ˆå¤šçš„å°å—ã€‚ç„¶åä½¿ç”¨è´ªå¿ƒç­–ç•¥ï¼Œè®¡ç®—æ¯ä¸¤ä¸ªç›¸é‚»çš„åŒºåŸŸçš„ç›¸ä¼¼åº¦ï¼Œç„¶åæ¯æ¬¡åˆå¹¶æœ€ç›¸ä¼¼çš„ä¸¤å—ï¼Œç›´è‡³æœ€ç»ˆåªå‰©ä¸‹ä¸€å—å®Œæ•´çš„å›¾ç‰‡ã€‚å¹¶å°†è¯¥è¿‡ç¨‹ä¸­æ¯æ¬¡äº§ç”Ÿçš„å›¾åƒå—åŒ…æ‹¬åˆå¹¶çš„å›¾åƒå—éƒ½ä¿å­˜ä¸‹æ¥ä½œä¸ºæœ€ç»ˆçš„**RoIï¼ˆRegion of Interestï¼‰é›†**ã€‚è¯¦ç»†ç®—æ³•æµç¨‹å¦‚ä¸‹ï¼š
![selective_search.png](https://cdn.nlark.com/yuque/0/2020/png/1626932/1596503573986-5040363d-86db-4db9-aacf-b75284c9e9e8.png#align=left&display=inline&height=503&margin=%5Bobject%20Object%5D&name=selective_search.png&originHeight=503&originWidth=590&size=143173&status=done&style=none&width=590#align=left&display=inline&height=503&margin=%5Bobject%20Object%5D&originHeight=503&originWidth=590&status=done&style=none&width=590)
åŒºåŸŸåˆå¹¶é‡‡ç”¨äº†å¤šæ ·æ€§çš„ç­–ç•¥ï¼Œå¦‚æœç®€å•é‡‡ç”¨ä¸€ç§ç­–ç•¥å¾ˆå®¹æ˜“é”™è¯¯åˆå¹¶ä¸ç›¸ä¼¼çš„åŒºåŸŸï¼Œæ¯”å¦‚åªè€ƒè™‘çº¹ç†æ—¶ï¼Œä¸åŒé¢œè‰²çš„åŒºåŸŸå¾ˆå®¹æ˜“è¢«è¯¯åˆå¹¶ã€‚selective searché‡‡ç”¨ä¸‰ç§å¤šæ ·æ€§ç­–ç•¥æ¥å¢åŠ å€™é€‰åŒºåŸŸä»¥ä¿è¯å¬å›ï¼š

- å¤šç§é¢œè‰²ç©ºé—´ï¼Œè€ƒè™‘RGBã€ç°åº¦ã€HSVåŠå…¶å˜ç§
- å¤šç§ç›¸ä¼¼åº¦åº¦é‡æ ‡å‡†ï¼Œæ—¢è€ƒè™‘é¢œè‰²ç›¸ä¼¼åº¦ï¼Œåˆè€ƒè™‘çº¹ç†ã€å¤§å°ã€é‡å æƒ…å†µç­‰
- é€šè¿‡æ”¹å˜é˜ˆå€¼åˆå§‹åŒ–åŸå§‹åŒºåŸŸï¼Œé˜ˆå€¼è¶Šå¤§ï¼Œåˆ†å‰²çš„åŒºåŸŸè¶Šå°‘

å¾ˆå¤šæœºå™¨å­¦ä¹ æ¡†æ¶éƒ½å†…ç½®å®ç°äº†selective searchæ“ä½œã€‚


### æ­¥éª¤äºŒ
è¯¥æ­¥éª¤ä¸­æˆ‘ä»¬è¦å®Œæˆçš„ç®—æ³•æµç¨‹éƒ¨åˆ†å¦‚ä¸‹å›¾æ•°å­—æ ‡è®°ï¼š
![RCNN_2.png](https://cdn.nlark.com/yuque/0/2020/png/1626932/1596503699136-9a50a47d-d4aa-4aaf-a08a-106cae72e5c6.png#align=left&display=inline&height=458&margin=%5Bobject%20Object%5D&name=RCNN_2.png&originHeight=458&originWidth=664&size=168728&status=done&style=none&width=664#align=left&display=inline&height=458&margin=%5Bobject%20Object%5D&originHeight=458&originWidth=664&status=done&style=none&width=664)
åœ¨æ­¥éª¤ä¸€ä¸­æˆ‘ä»¬å¾—åˆ°äº†ç”±**selective searchç®—æ³•**ç”Ÿæˆçš„**region proposals**ï¼Œä½†å„proposalå¤§å°åŸºæœ¬ä¸ä¸€è‡´ï¼Œè€ƒè™‘åˆ°**region proposals**åç»­è¦è¢«è¾“å…¥åˆ°**ConvNet**ä¸­è¿›è¡Œç‰¹å¾æå–ï¼Œå› æ­¤æœ‰å¿…è¦å°†æ‰€æœ‰**region proposals**è°ƒæ•´è‡³ç»Ÿä¸€ä¸”ç¬¦åˆ**ConvNet**æ¶æ„çš„æ ‡å‡†å°ºå¯¸ã€‚ç›¸å…³çš„ä»£ç å®ç°å¦‚ä¸‹ï¼š
```python
import matplotlib.patches as mpatches
# Clip Image
def clip_pic(img, rect):
    x = rect[0]
    y = rect[1]
    w = rect[2]
    h = rect[3]
    x_1 = x + w
    y_1 = y + h
    # return img[x:x_1, y:y_1, :], [x, y, x_1, y_1, w, h]   
    return img[y:y_1, x:x_1, :], [x, y, x_1, y_1, w, h]

#Resize Image
def resize_image(in_image, new_width, new_height, out_image=None, resize_mode=cv2.INTER_CUBIC):
    img = cv2.resize(in_image, (new_width, new_height), resize_mode)
    if out_image:
        cv2.imwrite(out_image, img)
    return img

def image_proposal(img_path):
    img = cv2.imread(img_path)
    img_lbl, regions = selective_search(
                       img, scale=500, sigma=0.9, min_size=10)
    candidates = set()
    images = []
    vertices = []
    for r in regions:
        # excluding same rectangle (with different segments)
        if r['rect'] in candidates:
            continue
        # excluding small regions
        if r['size'] < 220:
            continue
        if (r['rect'][2] * r['rect'][3]) < 500:
            continue
        # resize to 227 * 227 for input
        proposal_img, proposal_vertice = clip_pic(img, r['rect'])
        # Delete Empty array
        if len(proposal_img) == 0:
            continue
        # Ignore things contain 0 or not C contiguous array
        x, y, w, h = r['rect']
        if w == 0 or h == 0:
            continue
        # Check if any 0-dimension exist
        [a, b, c] = np.shape(proposal_img)
        if a == 0 or b == 0 or c == 0:
            continue
        resized_proposal_img = resize_image(proposal_img,224, 224)
        candidates.add(r['rect'])
        img_float = np.asarray(resized_proposal_img, dtype="float32")
        images.append(img_float)
        vertices.append(r['rect'])
    return images, vertices
```


è®©æˆ‘ä»¬é€‰æ‹©ä¸€å¼ å›¾ç‰‡æ£€æŸ¥ä¸‹selective searchç®—æ³•æ•ˆæœ
```python
img_path = './17flowers/jpg/7/image_0591.jpg' 
imgs, verts = image_proposal(img_path)
fig, ax = plt.subplots(ncols=1, nrows=1, figsize=(6, 6))
img = skimage.io.imread(img_path)
ax.imshow(img)
for x, y, w, h in verts:
    rect = mpatches.Rectangle((x, y), w, h, fill=False, edgecolor='red', linewidth=1)
    ax.add_patch(rect)
plt.show()
```


![1.png](https://cdn.nlark.com/yuque/0/2020/png/1626932/1596503832480-94125f09-0469-4324-8493-e4160dc83650.png#align=left&display=inline&height=361&margin=%5Bobject%20Object%5D&name=1.png&originHeight=361&originWidth=262&size=85828&status=done&style=none&width=262#align=left&display=inline&height=361&margin=%5Bobject%20Object%5D&originHeight=361&originWidth=262&status=done&style=none&width=262)
å¾—åˆ°å°ºå¯¸ç»Ÿä¸€çš„**proposals**åï¼Œå¯ä»¥å°†å…¶è¾“å…¥åˆ°**ConvNet**è¿›è¡Œç‰¹å¾æå–ã€‚è¿™é‡Œæˆ‘ä»¬**ConvNet**ä½¿ç”¨çš„ç½‘ç»œæ¶æ„æ¨¡å‹ä¸º**AlexNet**ã€‚å…¶ç½‘ç»œå…·ä½“æ„é€ å¦‚ä¸‹ï¼š
```python
import tflearn
from tflearn.layers.core import input_data, dropout, fully_connected
from tflearn.layers.conv import conv_2d, max_pool_2d
from tflearn.layers.normalization import local_response_normalization
from tflearn.layers.estimator import regression

# Building 'AlexNet'
def create_alexnet(num_classes, restore = True):
    # Building 'AlexNet'
    network = input_data(shape=[None, 224, 224, 3])
    network = conv_2d(network, 96, 11, strides=4, activation='relu')
    network = max_pool_2d(network, 3, strides=2)
    network = local_response_normalization(network)
    network = conv_2d(network, 256, 5, activation='relu')
    network = max_pool_2d(network, 3, strides=2)
    network = local_response_normalization(network)
    network = conv_2d(network, 384, 3, activation='relu')
    network = conv_2d(network, 384, 3, activation='relu')
    network = conv_2d(network, 256, 3, activation='relu')
    network = max_pool_2d(network, 3, strides=2)
    network = local_response_normalization(network)
    network = fully_connected(network, 4096, activation='tanh')
    network = dropout(network, 0.5)
    network = fully_connected(network, 4096, activation='tanh')
    network = dropout(network, 0.5)
    network = fully_connected(network, num_classes, activation='softmax', restore=restore)
    network = regression(network, optimizer='momentum',
                         loss='categorical_crossentropy',
                         learning_rate=0.001)
    return network
```


è‡³æ­¤ï¼Œæˆ‘ä»¬å®Œæˆäº†**ConvNet**éƒ¨åˆ†çš„æ¶æ„ï¼Œé€šè¿‡**ConvNet**æˆ‘ä»¬å¯ä»¥ä»**proposal**ä¸Šæå–åˆ°**feature map**ã€‚


### æ­¥éª¤ä¸‰ã€å››
è¯¥æ­¥éª¤ä¸­æˆ‘ä»¬è¦å®Œæˆçš„ç®—æ³•æµç¨‹éƒ¨åˆ†å¦‚ä¸‹å›¾æ•°å­—æ ‡è®°ï¼š
![image.png](https://cdn.nlark.com/yuque/0/2020/png/1626932/1596503916969-74edec04-1a48-4808-8a2a-1f9094f6ac9e.png#align=left&display=inline&height=458&margin=%5Bobject%20Object%5D&name=image.png&originHeight=458&originWidth=664&size=169086&status=done&style=none&width=664#align=left&display=inline&height=458&margin=%5Bobject%20Object%5D&originHeight=458&originWidth=664&status=done&style=none&width=664)
å¾—åˆ°æ¯ä¸ª**proposal**ä¸Šæå–åˆ°çš„**feature map**ä¹‹åï¼Œæˆ‘ä»¬å¯ä»¥å°†å…¶è¾“å…¥åˆ°**SVMs**ï¼ˆå€¼å¾—æ³¨æ„çš„æ˜¯SVMåˆ†ç±»å™¨çš„æ•°é‡å¹¶ä¸å”¯ä¸€ï¼Œæ¯å¯¹åº”ä¸€ä¸ªåˆ†ç±»ç±»åˆ«æˆ‘ä»¬éƒ½éœ€è¦è®­ç»ƒä¸€ä¸ªSVMã€‚å¯¹åº”åˆ°æˆ‘ä»¬çš„æ•°æ®é›†ï¼Œæœ€ç»ˆè¦åˆ†ç±»çš„èŠ±æœµç±»åˆ«æ˜¯ä¸¤ç±»ï¼Œå› æ­¤æ­¤æ—¶æˆ‘ä»¬çš„SVMæ•°é‡ä¸º2ä¸ªï¼‰ä¸­è¿›è¡Œ**åˆ†ç±»åˆ¤åˆ«**ã€‚
å¯¹äºä¸Šè¿°åˆ¤åˆ«ä¸ºæ­£ä¾‹ï¼ˆéèƒŒæ™¯ï¼‰çš„**proposal**åç»­è¾“å…¥åˆ°**Bbox reg**ä¸­è¿›è¡Œbboxçš„å¾®è°ƒï¼Œå¹¶è¾“å‡ºæœ€ç»ˆçš„è¾¹æ¡†é¢„æµ‹ã€‚
åœ¨çŸ¥æ™“äº†ç®—æ³•çš„æ•´ä¸ªæµç¨‹åï¼Œç°åœ¨è®©æˆ‘ä»¬ç€æ‰‹äºæ¨¡å‹è®­ç»ƒã€‚
## 
## æ¨¡å‹è®­ç»ƒ
R-CNNæ¨¡å‹çš„è®­ç»ƒåˆ†ä¸ºä¸¤æ­¥ï¼š

1. åˆå§‹åŒ–**ConvNet**å¹¶ä½¿ç”¨å¤§æ•°æ®é›†é¢„è®­ç»ƒå¾—åˆ°**é¢„è®­ç»ƒæ¨¡å‹**ï¼Œåœ¨**é¢„è®­ç»ƒæ¨¡å‹**ä¸Šä½¿ç”¨å°æ•°æ®é›†è¿›è¡Œfine-tuningå¹¶å¾—åˆ°æœ€ç»ˆçš„**ConvNet**ã€‚
1. å°†å›¾ç‰‡è¾“å…¥æ¨¡å‹ï¼Œé€šè¿‡ç¬¬ä¸€æ­¥ä¸­å¾—åˆ°çš„**ConvNet**æå–æ¯ä¸ªproposalçš„**feature map**ï¼Œä½¿ç”¨**feature map**æ¥è®­ç»ƒæˆ‘ä»¬çš„**åˆ†ç±»å™¨SVMs**å’Œ**å›å½’å™¨Bbox reg**ã€‚ï¼ˆè¯¥è¿‡ç¨‹**ConvNet**ä¸å‚ä¸å­¦ä¹ ï¼Œå³**ConvNet**çš„**å‚æ•°ä¿æŒä¸å˜**ï¼‰

é¦–å…ˆåœ¨å¤§æ•°æ®é›†ä¸Š**é¢„è®­ç»ƒ**,è®­ç»ƒæ—¶**è¾“å…¥X**ä¸º**åŸå›¾ç‰‡**ï¼Œ**æ­£ç¡®æ ‡ç­¾Y**ä¸º**åŸå›¾ç‰‡çš„åˆ†ç±»**ã€‚ç›¸å…³ä»£ç å¦‚ä¸‹ï¼š


```python
import codecs

def load_data(datafile, num_class, save=False, save_path='dataset.pkl'):
    fr = codecs.open(datafile, 'r', 'utf-8')
    train_list = fr.readlines()
    labels = []
    images = []
    for line in train_list:
        tmp = line.strip().split(' ')
        fpath = tmp[0]
        img = cv2.imread(fpath)
        img = resize_image(img, 224, 224)
        np_img = np.asarray(img, dtype="float32")
        images.append(np_img)

        index = int(tmp[1])
        label = np.zeros(num_class)
        label[index] = 1
        labels.append(label)
    if save:
        pickle.dump((images, labels), open(save_path, 'wb'))
    fr.close()
    return images, labels

def train(network, X, Y, save_model_path):
    # Training
    model = tflearn.DNN(network, checkpoint_path='model_alexnet',
                        max_checkpoints=1, tensorboard_verbose=2, tensorboard_dir='output')
    if os.path.isfile(save_model_path + '.index'):
        model.load(save_model_path)
        print('load model...')
    for _ in range(5):
        model.fit(X, Y, n_epoch=1, validation_set=0.1, shuffle=True,
                  show_metric=True, batch_size=64, snapshot_step=200,
                  snapshot_epoch=False, run_id='alexnet_oxflowers17') # epoch = 1000
        # Save the model
        model.save(save_model_path)
        print('save model...')
        
X, Y = load_data('./train_list.txt', 17)
net = create_alexnet(17)
train(net, X, Y,'./pre_train_model/model_save.model')
```


ä¹‹ååœ¨**é¢„è®­ç»ƒæ¨¡å‹**ä¸Šï¼Œä½¿ç”¨å°æ•°æ®é›†fine-tuningã€‚è¿™éƒ¨åˆ†è®­ç»ƒæ–¹å¼ä¸ä¸Šéƒ¨åˆ†è®­ç»ƒæœ‰ä¸¤ä¸ªä¸åŒç‚¹ï¼š
1.**è¾“å…¥**ä½¿ç”¨region proposalç”Ÿæˆçš„**RoI**è€Œä¸æ˜¯åŸå›¾ç‰‡ã€‚
2.å¯¹äºæ¯ä¸ªRoIçš„**æ­£ç¡®æ ‡ç­¾Y**ï¼Œæˆ‘ä»¬é€šè¿‡è®¡ç®—RoIä¸ground truthï¼ˆåŸå›¾ç‰‡æ ‡æ³¨çš„æ£€æµ‹ç‰©ä½“èŒƒå›´æ ‡ç­¾ï¼‰çš„**IOUï¼ˆIntersection over Unionï¼‰**æ¥ç¡®å®šã€‚
IoU**è®¡ç®—æ–¹å¼**å¦‚ä¸‹å›¾ï¼š

![image.png](https://cdn.nlark.com/yuque/0/2020/png/1626932/1596504189231-01fb60e3-6337-456f-9613-e0e7ce48732f.png#align=left&display=inline&height=342&margin=%5Bobject%20Object%5D&name=image.png&originHeight=342&originWidth=442&size=16362&status=done&style=none&width=442#align=left&display=inline&height=342&margin=%5Bobject%20Object%5D&originHeight=342&originWidth=442&status=done&style=none&width=442)
å¯çŸ¥IoUå–å€¼âˆˆ[0,1]ä¸”å–å€¼è¶Šå¤§è¡¨æ˜RoIä¸ground truthå·®è·è¶Šå°ã€‚ å®šä¹‰IoUå¤§äº0.5çš„å€™é€‰åŒºåŸŸä¸ºæ­£æ ·æœ¬ï¼Œå…¶ä½™çš„ä¸ºè´Ÿæ ·æœ¬ã€‚
è®¡ç®—IoUçš„ä»£ç å¦‚ä¸‹ï¼š
```python
# IOU Part 1
def if_intersection(xmin_a, xmax_a, ymin_a, ymax_a, xmin_b, xmax_b, ymin_b, ymax_b):
    if_intersect = False
    if xmin_a < xmax_b <= xmax_a and (ymin_a < ymax_b <= ymax_a or ymin_a <= ymin_b < ymax_a):
        if_intersect = True
    elif xmin_a <= xmin_b < xmax_a and (ymin_a < ymax_b <= ymax_a or ymin_a <= ymin_b < ymax_a):
        if_intersect = True
    elif xmin_b < xmax_a <= xmax_b and (ymin_b < ymax_a <= ymax_b or ymin_b <= ymin_a < ymax_b):
        if_intersect = True
    elif xmin_b <= xmin_a < xmax_b and (ymin_b < ymax_a <= ymax_b or ymin_b <= ymin_a < ymax_b):
        if_intersect = True
    else:
        return if_intersect
    if if_intersect:
        x_sorted_list = sorted([xmin_a, xmax_a, xmin_b, xmax_b])
        y_sorted_list = sorted([ymin_a, ymax_a, ymin_b, ymax_b])
        x_intersect_w = x_sorted_list[2] - x_sorted_list[1]
        y_intersect_h = y_sorted_list[2] - y_sorted_list[1]
        area_inter = x_intersect_w * y_intersect_h
        return area_inter


# IOU Part 2
def IOU(ver1, vertice2):
    # vertices in four points
    vertice1 = [ver1[0], ver1[1], ver1[0]+ver1[2], ver1[1]+ver1[3]]
    area_inter = if_intersection(vertice1[0], vertice1[2], vertice1[1], vertice1[3], vertice2[0], vertice2[2], vertice2[1], vertice2[3])
    if area_inter:
        area_1 = ver1[2] * ver1[3]
        area_2 = vertice2[4] * vertice2[5]
        iou = float(area_inter) / (area_1 + area_2 - area_inter)
        return iou
    return False
```


åœ¨ä½¿ç”¨å°æ•°æ®é›†è¿›è¡Œfine-tuningä¹‹å‰ï¼Œè®©æˆ‘ä»¬å®Œæˆç›¸å…³è®­ç»ƒæ•°æ®ï¼ˆRoIé›†çš„æ ‡ç­¾ã€å¯¹åº”å›¾ç‰‡ã€æ¡†ä½“æ ‡è®°ç­‰ï¼‰çš„è¯»å–å·¥ä½œï¼Œä¸‹æ–¹ä»£ç ä¸­æˆ‘ä»¬é¡ºå¸¦è¯»å–å¹¶ä¿å­˜äº†ç”¨äºSVMè®­ç»ƒå’Œç›®æ ‡æ¡†ä½“å›å½’çš„æ•°æ®ã€‚
```python
# Read in data and save data for Alexnet
def load_train_proposals(datafile, num_clss, save_path, threshold=0.5, is_svm=False, save=False):
    fr = open(datafile, 'r')
    train_list = fr.readlines()
    # random.shuffle(train_list)
    for num, line in enumerate(train_list):
        labels = []
        images = []
        rects = []
        tmp = line.strip().split(' ')
        # tmp0 = image address
        # tmp1 = label
        # tmp2 = rectangle vertices
        img = cv2.imread(tmp[0])
        # é€‰æ‹©æœç´¢å¾—åˆ°å€™é€‰æ¡†
        img_lbl, regions = selective_search(
                               img, scale=500, sigma=0.9, min_size=10)
        candidates = set()
        ref_rect = tmp[2].split(',')
        ref_rect_int = [int(i) for i in ref_rect]
        Gx = ref_rect_int[0]
        Gy = ref_rect_int[1]
        Gw = ref_rect_int[2]
        Gh = ref_rect_int[3]
        for r in regions:
            # excluding same rectangle (with different segments)
            if r['rect'] in candidates:
                continue
            # excluding small regions
            if r['size'] < 220:
                continue
            if (r['rect'][2] * r['rect'][3]) < 500:
                continue
            # æˆªå–ç›®æ ‡åŒºåŸŸ
            proposal_img, proposal_vertice = clip_pic(img, r['rect'])
            # Delete Empty array
            if len(proposal_img) == 0:
                continue
            # Ignore things contain 0 or not C contiguous array
            x, y, w, h = r['rect']
            if w == 0 or h == 0:
                continue
            # Check if any 0-dimension exist
            [a, b, c] = np.shape(proposal_img)
            if a == 0 or b == 0 or c == 0:
                continue
            resized_proposal_img = resize_image(proposal_img, 224, 224)
            candidates.add(r['rect'])
            img_float = np.asarray(resized_proposal_img, dtype="float32")
            images.append(img_float)
            # IOU
            iou_val = IOU(ref_rect_int, proposal_vertice)
            # x,y,w,hä½œå·®ï¼Œç”¨äºboundingboxå›å½’
            rects.append([(Gx-x)/w, (Gy-y)/h, math.log(Gw/w), math.log(Gh/h)])
            # propasal_rect = [proposal_vertice[0], proposal_vertice[1], proposal_vertice[4], proposal_vertice[5]]
            # print(iou_val)
            # labels, let 0 represent default class, which is background
            index = int(tmp[1])
            if is_svm:
                # iouå°äºé˜ˆå€¼ï¼Œä¸ºèƒŒæ™¯ï¼Œ0
                if iou_val < threshold:
                    labels.append(0)
                else:
                     labels.append(index)
            else:
                label = np.zeros(num_clss + 1)
                if iou_val < threshold:
                    label[0] = 1
                else:
                    label[index] = 1
                labels.append(label)


        if is_svm:
            ref_img, ref_vertice = clip_pic(img, ref_rect_int)
            resized_ref_img = resize_image(ref_img, 224, 224)
            img_float = np.asarray(resized_ref_img, dtype="float32")
            images.append(img_float)
            rects.append([0, 0, 0, 0])
            labels.append(index)
        view_bar("processing image of %s" % datafile.split('\\')[-1].strip(), num + 1, len(train_list))

        if save:
            if is_svm:
                # strip()å»é™¤é¦–ä½ç©ºæ ¼
                np.save((os.path.join(save_path, tmp[0].split('/')[-1].split('.')[0].strip()) + '_data.npy'), [images, labels, rects])
            else:
                # strip()å»é™¤é¦–ä½ç©ºæ ¼
                np.save((os.path.join(save_path, tmp[0].split('/')[-1].split('.')[0].strip()) + '_data.npy'),
                        [images, labels])
    print(' ')
    fr.close()
    
# load data
def load_from_npy(data_set):
    images, labels = [], []
    data_list = os.listdir(data_set)
    # random.shuffle(data_list)
    for ind, d in enumerate(data_list):
        i, l = np.load(os.path.join(data_set, d),allow_pickle=True)
        images.extend(i)
        labels.extend(l)
        view_bar("load data of %s" % d, ind + 1, len(data_list))
    print(' ')
    return images, labels

import math
import sys
#Progress bar 
def view_bar(message, num, total):
    rate = num / total
    rate_num = int(rate * 40)
    rate_nums = math.ceil(rate * 100)
    r = '\r%s:[%s%s]%d%%\t%d/%d' % (message, ">" * rate_num, " " * (40 - rate_num), rate_nums, num, total,)
    sys.stdout.write(r)
    sys.stdout.flush()
```


æœ‰äº†ä¸Šè¿°å‡†å¤‡æˆ‘ä»¬å¯ä»¥å¼€å§‹æ¨¡å‹fine-tuningé˜¶æ®µçš„è®­ç»ƒï¼Œç›¸å…³ä»£ç å¦‚ä¸‹ï¼š
```python
def fine_tune_Alexnet(network, X, Y, save_model_path, fine_tune_model_path):
    # Training
    model = tflearn.DNN(network, checkpoint_path='rcnn_model_alexnet',
                        max_checkpoints=1, tensorboard_verbose=2, tensorboard_dir='output_RCNN')
    if os.path.isfile(fine_tune_model_path + '.index'):
        print("Loading the fine tuned model")
        model.load(fine_tune_model_path)
    elif os.path.isfile(save_model_path + '.index'):
        print("Loading the alexnet")
        model.load(save_model_path)
    else:
        print("No file to load, error")
        return False

    model.fit(X, Y, n_epoch=1, validation_set=0.1, shuffle=True,
              show_metric=True, batch_size=64, snapshot_step=200,
              snapshot_epoch=False, run_id='alexnet_rcnnflowers2')
    # Save the model
    model.save(fine_tune_model_path)
        
data_set = './data_set'
if len(os.listdir('./data_set')) == 0:
    print("Reading Data")
    load_train_proposals('./fine_tune_list.txt', 2, save=True, save_path=data_set)
print("Loading Data")
X, Y = load_from_npy(data_set)
restore = False
if os.path.isfile('./fine_tune_model/fine_tune_model_save.model' + '.index'):
    restore = True
    print("Continue fine-tune")
# three classes include background
net = create_alexnet(3, restore=restore)
fine_tune_Alexnet(net, X, Y, './pre_train_model/model_save.model', './fine_tune_model/fine_tune_model_save.model')
```


### æ­¥éª¤äºŒ
è¯¥æ­¥éª¤ä¸­æˆ‘ä»¬è¦è®­ç»ƒ**SVMs**å’Œ**Bbox reg**å¦‚ä¸‹å›¾æ•°å­—æ ‡è®°ï¼š![image.png](https://cdn.nlark.com/yuque/0/2020/png/1626932/1596504298814-962b91dd-665d-4a41-89cc-1daaaa079ad7.png#align=left&display=inline&height=458&margin=%5Bobject%20Object%5D&name=image.png&originHeight=458&originWidth=664&size=168838&status=done&style=none&width=664#align=left&display=inline&height=458&margin=%5Bobject%20Object%5D&originHeight=458&originWidth=664&status=done&style=none&width=664)
é¦–å…ˆæˆ‘ä»¬ä»æ­¥éª¤ä¸€è¿™é‡Œä½¿ç”¨çš„CNNæ¨¡å‹é‡Œæå–å‡ºfeature mapï¼Œæ³¨æ„è¿™é‡Œä½¿ç”¨çš„**ConvNet**ä¸ä¹‹å‰è®­ç»ƒæ—¶æ‰€ç”¨çš„ç›¸æ¯”å°‘äº†æœ€åä¸€å±‚softmaxï¼Œå› ä¸ºæ­¤æ—¶æˆ‘ä»¬éœ€è¦çš„æ˜¯ä»RoIä¸Šæå–åˆ°çš„ç‰¹å¾è€Œè®­ç»ƒä¸­éœ€è¦softmaxå±‚æ¥è¿›è¡Œåˆ†ç±»ã€‚ç›¸å…³ä»£ç å¦‚ä¸‹ï¼š
```python
def create_alexnet():
    # Building 'AlexNet'
    network = input_data(shape=[None, 224, 224, 3])
    network = conv_2d(network, 96, 11, strides=4, activation='relu')
    network = max_pool_2d(network, 3, strides=2)
    network = local_response_normalization(network)
    network = conv_2d(network, 256, 5, activation='relu')
    network = max_pool_2d(network, 3, strides=2)
    network = local_response_normalization(network)
    network = conv_2d(network, 384, 3, activation='relu')
    network = conv_2d(network, 384, 3, activation='relu')
    network = conv_2d(network, 256, 3, activation='relu')
    network = max_pool_2d(network, 3, strides=2)
    network = local_response_normalization(network)
    network = fully_connected(network, 4096, activation='tanh')
    network = dropout(network, 0.5)
    network = fully_connected(network, 4096, activation='tanh')
    network = regression(network, optimizer='momentum',
                         loss='categorical_crossentropy',
                         learning_rate=0.001)
    return network
```


æ¯å¯¹åº”ä¸€ä¸ªåˆ†ç±»ç±»åˆ«æˆ‘ä»¬éƒ½éœ€è¦è®­ç»ƒä¸€ä¸ªSVMã€‚æˆ‘ä»¬æœ€ç»ˆè¦åˆ†ç±»çš„**èŠ±æœµç±»åˆ«æ˜¯ä¸¤ç±»**ï¼Œå› æ­¤æˆ‘ä»¬éœ€è¦è®­ç»ƒçš„**SVMæ•°é‡ä¸º2ä¸ª**ã€‚
SVMè®­ç»ƒæ‰€ç”¨çš„**è¾“å…¥**ä¸ºRoIä¸­æå–åˆ°çš„**feature map**ï¼Œæ‰€ç”¨çš„æ ‡ç­¾å…±æœ‰**n+1ä¸ªç±»åˆ«ï¼ˆ+1çš„ä¸ºèƒŒæ™¯ï¼‰**ï¼Œå¯¹åº”åˆ°æˆ‘ä»¬çš„æ•°æ®é›†æ­¤æ—¶æ ‡ç­¾å…±æœ‰**ä¸‰ä¸ªç±»åˆ«**ã€‚
ç›¸å…³ä»£ç å¦‚ä¸‹ï¼š
```python
from sklearn import svm
from sklearn.externals import joblib

# Construct cascade svms
def train_svms(train_file_folder, model):
    files = os.listdir(train_file_folder)
    svms = []
    train_features = []
    bbox_train_features = []
    rects = []
    for train_file in files:
        if train_file.split('.')[-1] == 'txt':
            X, Y, R = generate_single_svm_train(os.path.join(train_file_folder, train_file))
            Y1 = []
            features1 = []
            features_hard = []
            for ind, i in enumerate(X):
                # extract features æå–ç‰¹å¾
                feats = model.predict([i])
                train_features.append(feats[0])
                # æ‰€æœ‰æ­£è´Ÿæ ·æœ¬åŠ å…¥feature1,Y1
                if Y[ind]>=0:
                    Y1.append(Y[ind])
                    features1.append(feats[0])
                    # å¯¹ä¸groundtruthçš„iou>0.5çš„åŠ å…¥boundingboxè®­ç»ƒé›†
                    if Y[ind]>0:
                        bbox_train_features.append(feats[0])
                view_bar("extract features of %s" % train_file, ind + 1, len(X))

            clf = svm.SVC(probability=True)

            clf.fit(features1, Y1)
            print(' ')
            print("feature dimension")
            print(np.shape(features1))
            svms.append(clf)
            # å°†clfåºåˆ—åŒ–ï¼Œä¿å­˜svmåˆ†ç±»å™¨
            joblib.dump(clf, os.path.join(train_file_folder, str(train_file.split('.')[0]) + '_svm.pkl'))

    # ä¿å­˜boundingboxå›å½’è®­ç»ƒé›†
    np.save((os.path.join(train_file_folder, 'bbox_train.npy')),
            [bbox_train_features, rects])
    return svms

# Load training images
def generate_single_svm_train(train_file):
    save_path = train_file.rsplit('.', 1)[0].strip()
    if len(os.listdir(save_path)) == 0:
        print("reading %s's svm dataset" % train_file.split('\\')[-1])
        load_train_proposals(train_file, 2, save_path, threshold=0.3, is_svm=True, save=True)
    print("restoring svm dataset")
    images, labels,rects = load_from_npy_(save_path)

    return images, labels,rects

# load data
def load_from_npy_(data_set):
    images, labels ,rects= [], [], []
    data_list = os.listdir(data_set)
    # random.shuffle(data_list)
    for ind, d in enumerate(data_list):
        i, l, r = np.load(os.path.join(data_set, d),allow_pickle=True)
        images.extend(i)
        labels.extend(l)
        rects.extend(r)
        view_bar("load data of %s" % d, ind + 1, len(data_list))
    print(' ')
    return images, labels ,rects
```


å›å½’å™¨æ˜¯çº¿æ€§çš„ï¼Œè¾“å…¥ä¸ºNå¯¹å€¼ï¼Œ{(ğ‘ƒğ‘–,ğºğ‘–)}ğ‘–=1,2,â€¦,ğ‘{(Pi,Gi)}i=1,2,â€¦,Nï¼Œåˆ†åˆ«ä¸ºå€™é€‰åŒºåŸŸçš„æ¡†åæ ‡å’ŒçœŸå®çš„æ¡†åæ ‡ã€‚ç›¸å…³ä»£ç å¦‚ä¸‹ï¼š


```python
from sklearn.linear_model import Ridge

#åœ¨å›¾ç‰‡ä¸Šæ˜¾ç¤ºboundingbox
def show_rect(img_path, regions):
    fig, ax = plt.subplots(ncols=1, nrows=1, figsize=(6, 6))
    img = skimage.io.imread(img_path)
    ax.imshow(img)
    for x, y, w, h in regions:
        rect = mpatches.Rectangle(
            (x, y), w, h, fill=False, edgecolor='red', linewidth=1)
        ax.add_patch(rect)
    plt.show()
    

# è®­ç»ƒboundingboxå›å½’
def train_bbox(npy_path):
    features, rects = np.load((os.path.join(npy_path, 'bbox_train.npy')),allow_pickle=True)
    # ä¸èƒ½ç›´æ¥np.array()ï¼Œåº”è¯¥æŠŠå…ƒç´ å…¨éƒ¨å–å‡ºæ”¾å…¥ç©ºåˆ—è¡¨ä¸­ã€‚å› ä¸ºfeatureså’Œrectså»ºç«‹æ—¶ç”¨çš„appendï¼Œå¯¼è‡´å…¶ä¸­å…ƒç´ ç»“æ„ä¸èƒ½ç›´æ¥è½¬æ¢æˆçŸ©é˜µ
    X = []
    Y = []
    for ind, i in enumerate(features):
        X.append(i)
    X_train = np.array(X)

    for ind, i in enumerate(rects):
        Y.append(i)
    Y_train = np.array(Y)

    # çº¿æ€§å›å½’æ¨¡å‹è®­ç»ƒ
    clf = Ridge(alpha=1.0)
    clf.fit(X_train, Y_train)
    # åºåˆ—åŒ–ï¼Œä¿å­˜bboxå›å½’
    joblib.dump(clf, os.path.join(npy_path,'bbox_train.pkl'))
    return clf
```


å¼€å§‹è®­ç»ƒSVMåˆ†ç±»å™¨ä¸æ¡†ä½“å›å½’å™¨ã€‚
```python
train_file_folder = './svm_train'
# å»ºç«‹æ¨¡å‹ï¼Œç½‘ç»œ
net = create_alexnet()
model = tflearn.DNN(net)
# åŠ è½½å¾®è°ƒåçš„alexnetç½‘ç»œå‚æ•°
model.load('./fine_tune_model/fine_tune_model_save.model')
# åŠ è½½/è®­ç»ƒsvmåˆ†ç±»å™¨ å’Œ boundingboxå›å½’å™¨
svms = []
bbox_fit = []
# boundingboxå›å½’å™¨æ˜¯å¦æœ‰å­˜æ¡£
bbox_fit_exit = 0
# åŠ è½½svmåˆ†ç±»å™¨å’Œboundingboxå›å½’å™¨
for file in os.listdir(train_file_folder):
    if file.split('_')[-1] == 'svm.pkl':
        svms.append(joblib.load(os.path.join(train_file_folder, file)))
    if file == 'bbox_train.pkl':
        bbox_fit = joblib.load(os.path.join(train_file_folder, file))
        bbox_fit_exit = 1
if len(svms) == 0:
    svms = train_svms(train_file_folder, model)
if bbox_fit_exit == 0:
    bbox_fit = train_bbox(train_file_folder)

print("Done fitting svms")
```
è‡³æ­¤æ¨¡å‹å·²è®­ç»ƒå®Œæ¯•ã€‚

## 
## æ¨¡å‹æ•ˆæœæŸ¥çœ‹
è®©æˆ‘ä»¬é€‰æ‹©ä¸€å¼ å›¾ç‰‡é¡ºç€æ¨¡å‹æ­£å‘ä¼ æ’­çš„é¡ºåºæŸ¥çœ‹æ¨¡å‹çš„å…·ä½“è¿è¡Œæ•ˆæœã€‚é¦–å…ˆæŸ¥çœ‹ä¸‹region proposalæ‰€äº§ç”Ÿçš„RoIåŒºåŸŸã€‚


```python
img_path = './2flowers/jpg/1/image_1282.jpg'  
image = cv2.imread(img_path)
im_width = image.shape[1]
im_height = image.shape[0]
# æå–region proposal
imgs, verts = image_proposal(img_path)
show_rect(img_path, verts)
```


![2.png](https://cdn.nlark.com/yuque/0/2020/png/1626932/1596504699313-82dd2fb6-6c9d-47f0-9932-7aff623af3a4.png#align=left&display=inline&height=361&margin=%5Bobject%20Object%5D&name=2.png&originHeight=361&originWidth=352&size=118903&status=done&style=none&width=352#align=left&display=inline&height=361&margin=%5Bobject%20Object%5D&originHeight=361&originWidth=352&status=done&style=none&width=352)
å°†RoIè¾“å…¥ConvNetä¸­å¾—åˆ°ç‰¹å¾å¹¶è¾“å…¥SVMsä¸­ä¸å›å½’å™¨ä¸­ï¼Œå¹¶é€‰å–SVMåˆ†ç±»ç»“æœä¸ºæ­£ä¾‹çš„æ ·ä¾‹è¿›è¡Œè¾¹æ¡†å›å½’ã€‚


```python
# ä»CNNä¸­æå–RoIçš„ç‰¹å¾
features = model.predict(imgs)
print("predict image:")
# print(np.shape(features))
results = []
results_label = []
results_score = []
count = 0
print(len(features))
for f in features:
    for svm in svms:
        pred = svm.predict([f.tolist()])
        # not background
        if pred[0] != 0:
            # boundingboxå›å½’
            bbox = bbox_fit.predict([f.tolist()])
            tx, ty, tw, th = bbox[0][0], bbox[0][1], bbox[0][2], bbox[0][3]
            px, py, pw, ph = verts[count]
            gx = tx * pw + px
            gy = ty * ph + py
            gw = math.exp(tw) * pw
            gh = math.exp(th) * ph
            if gx < 0:
                gw = gw - (0 - gx)
                gx = 0
            if gx + gw > im_width:
                gw = im_width - gx
            if gy < 0:
                gh = gh - (0 - gh)
                gy = 0
            if gy + gh > im_height:
                gh = im_height - gy
            results.append([gx, gy, gw, gh])
            results_label.append(pred[0])
            results_score.append(svm.predict_proba([f.tolist()])[0][1])
    count += 1
print(results)
print(results_label)
print(results_score)
show_rect(img_path, results)
```


![3.png](https://cdn.nlark.com/yuque/0/2020/png/1626932/1596504731508-134d32d2-b89c-43ce-a8db-5af305739e67.png#align=left&display=inline&height=361&margin=%5Bobject%20Object%5D&name=3.png&originHeight=361&originWidth=352&size=119688&status=done&style=none&width=352#align=left&display=inline&height=361&margin=%5Bobject%20Object%5D&originHeight=361&originWidth=352&status=done&style=none&width=352)
å¯ä»¥çœ‹åˆ°å¯èƒ½ä¼šå¾—åˆ°æ•°é‡å¤§äºä¸€çš„æ¡†ä½“ï¼Œæ­¤æ—¶æˆ‘ä»¬éœ€è¦å€ŸåŠ©NMSï¼ˆNon-Maximum Suppressionï¼‰æ¥é€‰æ‹©å‡ºç›¸å¯¹æœ€ä¼˜çš„ç»“æœã€‚
ä»£ç å¦‚ä¸‹ï¼š


```python
results_final = []
results_final_label = []

# éæå¤§æŠ‘åˆ¶
# åˆ é™¤å¾—åˆ†å°äº0.5çš„å€™é€‰æ¡†
delete_index1 = []
for ind in range(len(results_score)):
    if results_score[ind] < 0.5:
        delete_index1.append(ind)
num1 = 0
for idx in delete_index1:
    results.pop(idx - num1)
    results_score.pop(idx - num1)
    results_label.pop(idx - num1)
    num1 += 1

while len(results) > 0:
    # æ‰¾åˆ°åˆ—è¡¨ä¸­å¾—åˆ†æœ€é«˜çš„
    max_index = results_score.index(max(results_score))
    max_x, max_y, max_w, max_h = results[max_index]
    max_vertice = [max_x, max_y, max_x + max_w, max_y + max_h, max_w, max_h]
    # è¯¥å€™é€‰æ¡†åŠ å…¥æœ€ç»ˆç»“æœ
    results_final.append(results[max_index])
    results_final_label.append(results_label[max_index])
    # ä»resultsä¸­åˆ é™¤è¯¥å€™é€‰æ¡†
    results.pop(max_index)
    results_label.pop(max_index)
    results_score.pop(max_index)
    # print(len(results_score))
    # åˆ é™¤ä¸å¾—åˆ†æœ€é«˜å€™é€‰æ¡†iou>0.5çš„å…¶ä»–å€™é€‰æ¡†
    delete_index = []
    for ind, i in enumerate(results):
        iou_val = IOU(i, max_vertice)
        if iou_val > 0.5:
            delete_index.append(ind)
    num = 0
    for idx in delete_index:
        # print('\n')
        # print(idx)
        # print(len(results))
        results.pop(idx - num)
        results_score.pop(idx - num)
        results_label.pop(idx - num)
        num += 1

print("result:",results_final)
print("result label:",results_final_label)
show_rect(img_path, results_final)
```


![image.png](https://cdn.nlark.com/yuque/0/2020/png/1626932/1596504783108-3ceaa71c-1e18-4e33-84ec-41ff24a094d3.png#align=left&display=inline&height=361&margin=%5Bobject%20Object%5D&name=image.png&originHeight=361&originWidth=352&size=121789&status=done&style=none&width=352#align=left&display=inline&height=361&margin=%5Bobject%20Object%5D&originHeight=361&originWidth=352&status=done&style=none&width=352)

# æ€»ç»“
è‡³æ­¤æˆ‘ä»¬å¾—åˆ°äº†ä¸€ä¸ªç²—ç³™çš„R-CNNæ¨¡å‹ã€‚
R-CNNçµæ´»åœ°è¿ç”¨äº†å½“æ—¶æ¯”è¾ƒå…ˆè¿›çš„å·¥å…·å’ŒæŠ€æœ¯ï¼Œå¹¶å……åˆ†å¸æ”¶ï¼Œæ ¹æ®è‡ªå·±çš„é€»è¾‘æ”¹é€ ï¼Œæœ€ç»ˆå–å¾—äº†å¾ˆå¤§çš„è¿›æ­¥ã€‚ä½†å…¶ä¸­ä¹Ÿæœ‰ä¸å°‘æ˜æ˜¾çš„ç¼ºç‚¹ï¼š
1. è®­ç»ƒè¿‡äºç¹çï¼šå¾®è°ƒç½‘ç»œ+è®­ç»ƒSVM+è¾¹æ¡†å›å½’ï¼Œå…¶ä¸­ä¼šæ¶‰åŠåˆ°è®¸å¤šç¡¬ç›˜è¯»å†™æ“ä½œæ•ˆç‡ä½ä¸‹ã€‚
2. æ¯ä¸ªRoIéƒ½éœ€è¦ç»è¿‡CNNç½‘ç»œè¿›è¡Œç‰¹å¾æå–ï¼Œäº§ç”Ÿäº†å¤§é‡çš„é¢å¤–è¿ç®—ï¼ˆæƒ³è±¡ä¸€ä¸‹ä¸¤ä¸ªæœ‰é‡åˆéƒ¨åˆ†çš„RoIï¼Œé‡åˆéƒ¨åˆ†ç›¸å½“äºè¿›è¡Œäº†ä¸¤æ¬¡å·ç§¯è¿ç®—ï¼Œä½†ç†è®ºä¸Šæ¥è¯´ä»…éœ€è¿›è¡Œä¸€æ¬¡ï¼‰ã€‚
3. è¿è¡Œé€Ÿåº¦æ…¢ï¼Œåƒç‹¬ç«‹ç‰¹å¾æå–ã€ä½¿ç”¨selective searchä½œä¸ºregion proposalç­‰éƒ½è¿‡äºè€—æ—¶ã€‚
å¹¸è¿çš„æ˜¯ï¼Œè¿™äº›é—®é¢˜åœ¨åç»­çš„Fast R-CNNä¸Faster R-CNNéƒ½æœ‰äº†å¾ˆå¤§çš„æ”¹å–„ã€‚

## 
## é¡¹ç›®åœ°å€
[https://momodel.cn/workspace/5f1ec0505607a4070d65203b?type=app](https://momodel.cn/workspace/5f1ec0505607a4070d65203b?type=app)






