# 专栏2-激活函数（一）浅谈激活函数以及其发展

激活函数是神经网络的相当重要的一部分，在神经网络的发展史上，各种激活函数也是一个研究的方向。我们在学习中，往往没有思考过——为什么用这个函数以及它们是从何而来？


![](https://cdn.nlark.com/yuque/0/2020/jpeg/1626951/1598705956922-2b395354-cf37-4e1f-b4b7-bb8ce8ae8cc3.jpeg#align=left&display=inline&height=229&margin=%5Bobject%20Object%5D&originHeight=229&originWidth=400&size=0&status=done&style=none&width=400)


生物神经网络曾给予了人工神经网络相当多的启发。如上图，来自树突信号不断累积，如若信号强度超过一个特定阈值，则向轴突继续传递信号。如若未超过，则该信号被神经元“杀死”，无法继续传播。


在人工神经网络之中，激活函数有着异曲同工之妙。试想，当我们学习了一些新的东西之后，一些神经元会产生不同的输出信号，这使得神经元得以连接。


sigmoid函数也许是大家初学神经网络时第一个接触到的激活函数，我们知道它有很多良好的特性，诸如能将连续的实值变换为0到1的输出、求导简单，那么这个函数是怎么得到的呢？本文从最大熵原理提供一个角度。


## 1 sigmoid函数与softmax函数


### 1.1 最大熵原理与模型


最大熵原理是概率模型学习的一个准则[](#refer-anchor)。最大熵原理认为，学习概率模型时，在所有可能的概率模型中，熵最大的模型是最好的模型。


假设离散随机变量![](https://cdn.nlark.com/yuque/__latex/02129bb861061d1a052c592e2dc6b383.svg#card=math&code=X&height=17&width=16)的概率分布是![](https://cdn.nlark.com/yuque/__latex/0c3d72395d7576ab13b9e9389f865960.svg#card=math&code=P%28X%29&height=22&width=44)，则其熵是
![](https://cdn.nlark.com/yuque/__latex/8f1b4c49cce517b24ad56f6e9993af2d.svg#card=math&code=H%28P%29%3D-%5Csum_%7Bx%7DP%28x%29%5Clog%20P%28x%29&height=44&width=226)
熵满足下列不等式：
![](https://cdn.nlark.com/yuque/__latex/265923ed18650faf71dbd199e79df440.svg#card=math&code=0%5Cleqslant%20H%28P%29%5Cleqslant%20%5Clog%20%7CX%7C&height=22&width=157)
式中，![](https://cdn.nlark.com/yuque/__latex/e4240ff886dbbfcedd3db989e33b4853.svg#card=math&code=%7CX%7C&height=22&width=26)是![](https://cdn.nlark.com/yuque/__latex/02129bb861061d1a052c592e2dc6b383.svg#card=math&code=X&height=17&width=16)的取值个数，当且仅当![](https://cdn.nlark.com/yuque/__latex/02129bb861061d1a052c592e2dc6b383.svg#card=math&code=X&height=17&width=16)的分布是均匀分布时右边的等号成立。这就是说，当![](https://cdn.nlark.com/yuque/__latex/02129bb861061d1a052c592e2dc6b383.svg#card=math&code=X&height=17&width=16)服从均匀分布时，熵最大。


直观而言，此原理认为要选择的概率模型首先必须满足已有的条件，在无更多信息的条件下没其他不确定的部分都是等可能的。


假设分类模型是一个条件概率分布![](https://cdn.nlark.com/yuque/__latex/a8c14bbd3415899e56075e656f90cb72.svg#card=math&code=P%28Y%5Cmid%20X%29&height=22&width=74)，给定一个训练集![](https://cdn.nlark.com/yuque/__latex/e5c8c08678420c4ed8772431edd1ee2f.svg#card=math&code=T%3D%5Cleft%5C%7B%28x_1%2Cy_1%29%2C%28x_2%2Cy_2%29%2C%5Ccdots%2C%28x_N%2Cy_N%29%5Cright%5C%7D&height=22&width=296)，可以确定![](https://cdn.nlark.com/yuque/__latex/eb58c4cfd17b18317dbf1ff80dd5945c.svg#card=math&code=P%28X%2C%20Y%29&height=22&width=66)的经验分布和边缘分布![](https://cdn.nlark.com/yuque/__latex/0c3d72395d7576ab13b9e9389f865960.svg#card=math&code=P%28X%29&height=22&width=44)的经验分布，分别以![](https://cdn.nlark.com/yuque/__latex/8322551529aade877b571c9792bb8b1e.svg#card=math&code=%5Ctilde%7BP%7D%28X%2CY%29&height=25&width=67)和![](https://cdn.nlark.com/yuque/__latex/f1be63e1f3c27ea3bd0a985bd77b5c31.svg#card=math&code=%5Ctilde%7BP%7D%28X%29&height=25&width=45)表示。


用特征函数（feature function）![](https://cdn.nlark.com/yuque/__latex/3baf1600ae50930a155f58ae172b51bd.svg#card=math&code=f%28x%2Cy%29&height=22&width=53)描述输入![](https://cdn.nlark.com/yuque/__latex/9dd4e461268c8034f5c8564e155c67a6.svg#card=math&code=x&height=13&width=10)和![](https://cdn.nlark.com/yuque/__latex/415290769594460e2e485922904f345d.svg#card=math&code=y%0A&height=16&width=9)之间的某一个事实，定义为：
![](https://cdn.nlark.com/yuque/__latex/08d36943dfde1f45f46932c01507bd53.svg#card=math&code=f%28x%2C%20y%29%3D%5Cleft%5C%7B%5Cbegin%7Barray%7D%7Bll%7D%0A1%2C%20%26%20x%20%5Ctext%20%7B%20%E4%B8%8E%20%7D%20y%20%5Ctext%20%7B%20%E6%BB%A1%E8%B6%B3%E6%9F%90%E4%B8%80%E4%BA%8B%E5%AE%9E%20%7D%20%5C%5C%0A0%2C%20%26%20%5Ctext%20%7B%20%E5%90%A6%E5%88%99%20%7D%0A%5Cend%7Barray%7D%5Cright.&height=60&width=292)
由上述信息，可以假设![](https://cdn.nlark.com/yuque/__latex/3baf1600ae50930a155f58ae172b51bd.svg#card=math&code=f%28x%2Cy%29&height=22&width=53)关于经验分布![](https://cdn.nlark.com/yuque/__latex/8322551529aade877b571c9792bb8b1e.svg#card=math&code=%5Ctilde%7BP%7D%28X%2CY%29&height=25&width=67)的期望值和关于模型![](https://cdn.nlark.com/yuque/__latex/a8c14bbd3415899e56075e656f90cb72.svg#card=math&code=P%28Y%5Cmid%20X%29&height=22&width=74)与经验分布![](https://cdn.nlark.com/yuque/__latex/f1be63e1f3c27ea3bd0a985bd77b5c31.svg#card=math&code=%5Ctilde%7BP%7D%28X%29&height=25&width=45)的期望值相等，即：
![](https://cdn.nlark.com/yuque/__latex/b2f93ffc8d9126fb7bee973626a17030.svg#card=math&code=%5Csum%7Bx%2C%20y%7D%20%5Ctilde%7BP%7D%28x%2C%20y%29%20f%7Bi%7D%28x%2C%20y%29%3D%5Csum%7Bx%2C%20y%7D%20%5Ctilde%7BP%7D%28x%29%20P%28y%20%5Cmid%20x%29%20f%7Bi%7D%28x%2C%20y%29&height=30&width=422)
结合条件，该问题等价于约束最优化问题：
![](https://cdn.nlark.com/yuque/__latex/66b336b3074b85e4755018c122ca30c1.svg#card=math&code=%5Cbegin%7Baligned%7D%0A%26%5Cmin%20_%7BP%20%5Cin%20%5Cmathbf%7BC%7D%7D%20-%20H%28P%29%3D%5Csum_%7Bx%2C%20y%7D%20%5Ctilde%7BP%7D%28x%29%20P%28y%20%5Cmid%20x%29%20%5Clog%20P%28y%20%5Cmid%20x%29%5C%5C%0A%5Ctext%20%7B%20s.t.%20%7D%20%26%5Cquad%20E_%7BP%7D%5Cleft%28f_%7Bi%7D%5Cright%29%3DE_%7B%5Ctilde%7BP%7D%7D%5Cleft%28f_%7Bi%7D%5Cright%29%2C%20%5Cquad%20i%3D1%2C2%2C%20%5Ccdots%2C%20n%5C%5C%0A%26%5Csum_%7By%7D%20P%28y%20%5Cmid%20x%29%3D1%0A%5Cend%7Baligned%7D&height=119&width=391)
由拉格朗日乘子法，问题转换为求如下式子的最小值
![](https://cdn.nlark.com/yuque/__latex/3db87c464323e3f910361793e81ee12d.svg#card=math&code=%5Cbegin%7Bequation%7D%0AL%3D%5Csum_%7Bx%2C%20y%7D%20%5Ctilde%7BP%7D%28x%29%20P%28y%20%5Cmid%20x%29%20%5Cln%20P%28y%20%5Cmid%20x%29-w_%7B0%7D%5Cleft%28%5Csum_%7By%7D%20P%28y%20%5Cmid%20x%29-1%5Cright%29-%5Csum_%7Bi%7D%20w_%7Bi%7D%5Cleft%28%5Csum_%7Bx%2C%20y%7D%20%5Ctilde%7BP%7D%28x%2C%20y%29%20f_%7Bi%7D%28x%2C%20y%29%5Cright.%0A%5Cleft.-%5Csum_%7Bx%2C%20y%7D%20%5Ctilde%7BP%7D%28x%29%20P%28y%20%5Cmid%20x%29%20f_%7Bi%7D%28x%2C%20y%29%5Cright%29%0A%5Cend%7Bequation%7D&height=61&width=926)
此时，我们对![](https://cdn.nlark.com/yuque/__latex/d20caec3b48a1eef164cb4ca81ba2587.svg#card=math&code=L%0A&height=17&width=12)求![](https://cdn.nlark.com/yuque/__latex/7082016653a57c45dae6f7d57ceea8ba.svg#card=math&code=P%28Y%7CX%29&height=22&width=64)的导数：
![](https://cdn.nlark.com/yuque/__latex/c4927c0330acea3c6a80a5a7a8a341f0.svg#card=math&code=%5Cbegin%7Baligned%7D%0A%5Cfrac%7B%5Cpartial%7BL%7D%7D%7B%5Cpartial%20P%28y%20%5Cmid%20x%29%7D%20%26%3D%5Csum_%7Bx%2Cy%7D%5Ctilde%7BP%7D%28x%29%28%5Clog%20P%28y%20%5Cmid%20x%29%2B1%29-%5Csum_%7Bx%2Cy%7Dw_%7B0%7D%2B%5Csum_%7Bi%7D%20w_%7Bi%7D%28x%29%20%5Ctilde%7BP%7D%28x%29%20f_%7Bi%7D%28x%2C%20y%29%20%5C%5C%0A%26%3D%5Csum_%7Bx%2Cy%7D%5Ctilde%7BP%7D%28x%29%5Cleft%28%28%5Clog%20P%28y%20%5Cmid%20x%29%2B1%29-w_%7B0%7D%2B%5Csum_%7Bi%7D%20w_%7Bi%7D%28x%29%20f_%7Bi%7D%28x%2C%20y%29%5Cright%29%0A%5Cend%7Baligned%7D&height=116&width=591)
令其导数值为0，在![](https://cdn.nlark.com/yuque/__latex/d99cfe9ff0c39d7bf65af8d9c054b79e.svg#card=math&code=%5Ctilde%7BP%7D%28X%29%20%3E%200&height=25&width=79)的情况下，解得：
![](https://cdn.nlark.com/yuque/__latex/549341876c8ac1a3a498ffeff1e82ac4.svg#card=math&code=P%28y%5Cmid%20x%29%3D%20%5Cexp%20%5Cleft%28%20%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%20w_if_i%28x%2Cy%29%2Bw_0-1%20%5Cright%29&height=60&width=332)
由于![](https://cdn.nlark.com/yuque/__latex/c80656c040357825becedcb7cb5dd1e1.svg#card=math&code=%5Csum_%7By%7DP%28y%5Cmid%20x%29%3D1&height=46&width=128)，得：
![](https://cdn.nlark.com/yuque/__latex/6ec7148cb7c7dfff6415b8686ca98fd8.svg#card=math&code=%5Cbegin%7Baligned%7D%0A%5Csum_%7By%7DP%28y%5Cmid%20x%29%20%26%3D%20%5Csum_%7By%7D%5Cexp%20%5Cleft%28%20%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%20w_if_i%28x%2Cy%29%2Bw_0-1%20%5Cright%29%20%5C%5C%0A%26%3D%5Cfrac%7B%5Csum_%7By%7D%5Cexp%20%5Cleft%28%20%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%20w_if_i%28x%2Cy%29%20%5Cright%29%7D%7B%5Cexp%281-w_0%29%7D%20%3D%201%0A%5Cend%7Baligned%7D&height=119&width=399)
由上面两式可得：
![](https://cdn.nlark.com/yuque/__latex/f39166152a954c76016ae4f41e7ab31c.svg#card=math&code=P%28y%20%5Cmid%20x%29%3D%5Cfrac%7Be%5E%7B-%5Csum_%7Bi%7D%20w_%7Bi%7D%20f_%7Bi%7D%28x%2C%20y%29%7D%7D%7B%5Csum_%7By%7D%20e%5E%7B-%5Csum_%7Bi%7D%20w_%7Bi%7D%20f_%7Bi%7D%28x%2C%20y%29%7D%7D&height=57&width=227)
细心的同学不难发现，这和softmax函数十分相近，定义![](https://cdn.nlark.com/yuque/__latex/b45968e3bb6aced8b1a147b64a0d31fa.svg#card=math&code=f_i%28x%2Cy%29%3Dx&height=22&width=94)，即可得到softmax函数：
![](https://cdn.nlark.com/yuque/__latex/4ed9b92542efd1ccb687a91184d6e5ea.svg#card=math&code=P%28y%20%5Cmid%20x%29%3D%5Cfrac%7Be%5E%7B-%5Csum_%7Bi%7D%20w_%7Bi%7D%20x%7D%7D%7B%5Csum_%7By%7D%20e%5E%7B-%5Csum_%7Bi%7D%20w_%7Bi%7D%20x%7D%7D&height=57&width=195)
那么sigmoid函数呢？其实该函数就是softmax函数的二分类特例：
![](https://cdn.nlark.com/yuque/__latex/0539b7a495f303bedbe58e02cf16177d.svg#card=math&code=P%28y%3D1%20%5Cmid%20x%29%3D%5Cfrac%7B1%7D%7B1%2Be%5E%7B%5Csum_%7Bi%7D%20w_%7Bi%7D%20x%7D%7D&height=46&width=217)
说完了推导，就来谈谈这两函数的特点。sigmoid函数的优点前文已提到，但sigmoid在反向传播时容易出现“梯度消失”的现象。


![](https://cdn.nlark.com/yuque/0/2020/png/1626951/1598705956978-d5c77111-8293-4191-98a9-0f0db67929c8.png#align=left&display=inline&height=277&margin=%5Bobject%20Object%5D&originHeight=277&originWidth=372&size=0&status=done&style=none&width=372)


可以看出，当输入值很大或很小时，其导数接近于0，它会导致梯度过小无法训练。


## 2 ReLU函数族的崛起


![](https://cdn.nlark.com/yuque/0/2020/png/1626951/1598705956930-5da620dc-d34a-447d-b601-a45f1d4ddffc.png#align=left&display=inline&height=277&margin=%5Bobject%20Object%5D&originHeight=277&originWidth=368&size=0&status=done&style=none&width=368)


如图所示，ReLU函数很好避免的梯度消失的问题，与Sigmoid/tanh函数相比，ReLU激活函数的优点是：


- 使用梯度下降（GD）法时，收敛速度更快 。
- 相比ReLU只需要一个门限值，即可以得到激活值，计算速度更快 。
缺点是：  ReLU的输入值为负的时候，输出始终为0，其一阶导数也始终为0，这样会导致神经元不能更新参数，也就是神经元不学习了，这种现象叫做“Dead Neuron”。



为了解决ReLU函数这个缺点，又出现了不少基于ReLU函数的发展，比如Leaky ReLU(带泄漏单元的ReLU)、 RReLU（随机ReLU）等等，也许你有一天也能发现效果更好的ReLU函数呢！




## 引用


[1] [李航. 统计学习方法[M]. 清华大学出版社, 2012.]
